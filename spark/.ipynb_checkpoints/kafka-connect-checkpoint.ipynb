{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's integrate a Kafka stream with a Spark session\n",
    "\n",
    "From the ./kafka_2.10-0.10.0.1 directory, as downloaded and extracted\n",
    "\n",
    "in one terminal, start the zookeeper server <br>\n",
    "$ bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "\n",
    "in another, start the kafka server <br>\n",
    "$ bin/kafka-server-start.sh config/server.properties <br>\n",
    "\n",
    "in another, run the producer with some topic, let's use 'kafka-test' here <br>\n",
    "$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic kafka-test <br>\n",
    "\n",
    "in another, run the consumer <br>\n",
    "$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic kafka-test --from-beginning\n",
    "\n",
    "typing into the console of the producer, will be send to the consumer right away and 'consumed' to appear in the 'consumer' console <br>\n",
    "\n",
    "Now in the spark directory..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    ">>> spark\n",
    "<pyspark.sql.session.SparkSession object at 0x103c48fd0>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try connecting to the kafka stream with these steps http://spark.apache.org/docs/latest/streaming-kafka-integration.html#approach-2-direct-approach-no-receivers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE             RELEASE             \u001b[1m\u001b[36mexamples\u001b[m\u001b[m            \u001b[1m\u001b[36mlogs\u001b[m\u001b[m\r\n",
      "NOTICE              \u001b[1m\u001b[36mbin\u001b[m\u001b[m                 \u001b[1m\u001b[36mjars\u001b[m\u001b[m                \u001b[1m\u001b[36mpython\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[36mR\u001b[m\u001b[m                   \u001b[1m\u001b[36mconf\u001b[m\u001b[m                kafka-connect.ipynb \u001b[1m\u001b[36msbin\u001b[m\u001b[m\r\n",
      "README.md           \u001b[1m\u001b[36mdata\u001b[m\u001b[m                \u001b[1m\u001b[36mlicenses\u001b[m\u001b[m            \u001b[1m\u001b[36myarn\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./sbin/start-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Command: /Library/Java/JavaVirtualMachines/jdk1.8.0_92.jdk/Contents/Home/jre/bin/java -cp /Users/Alexandre/Workstation/spark-2.0.0-bin-hadoop2.7/conf/:/Users/Alexandre/Workstation/spark-2.0.0-bin-hadoop2.7/jars/* -Xmx1g org.apache.spark.deploy.master.Master --host Alexandres-MBP.lan --port 7077 --webui-port 8080\r\n",
      "========================================\r\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\r\n",
      "16/09/14 13:57:16 INFO Master: Started daemon with process name: 36731@Alexandres-MBP.lan\r\n",
      "16/09/14 13:57:16 INFO SignalUtils: Registered signal handler for TERM\r\n",
      "16/09/14 13:57:16 INFO SignalUtils: Registered signal handler for HUP\r\n",
      "16/09/14 13:57:16 INFO SignalUtils: Registered signal handler for INT\r\n",
      "16/09/14 13:57:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n",
      "16/09/14 13:57:17 INFO SecurityManager: Changing view acls to: Alexandre\r\n",
      "16/09/14 13:57:17 INFO SecurityManager: Changing modify acls to: Alexandre\r\n",
      "16/09/14 13:57:17 INFO SecurityManager: Changing view acls groups to: \r\n",
      "16/09/14 13:57:17 INFO SecurityManager: Changing modify acls groups to: \r\n",
      "16/09/14 13:57:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(Alexandre); groups with view permissions: Set(); users  with modify permissions: Set(Alexandre); groups with modify permissions: Set()\r\n",
      "16/09/14 13:57:17 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\r\n",
      "16/09/14 13:57:17 INFO Master: Starting Spark master at spark://Alexandres-MBP.lan:7077\r\n",
      "16/09/14 13:57:17 INFO Master: Running Spark version 2.0.0\r\n",
      "16/09/14 13:57:18 INFO Utils: Successfully started service 'MasterUI' on port 8080.\r\n",
      "16/09/14 13:57:18 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://10.255.165.198:8080\r\n",
      "16/09/14 13:57:18 INFO Utils: Successfully started service on port 6066.\r\n",
      "16/09/14 13:57:18 INFO StandaloneRestServer: Started REST server for submitting applications on port 6066\r\n",
      "16/09/14 13:57:18 INFO Master: I have been elected leader! New state: ALIVE\r\n"
     ]
    }
   ],
   "source": [
    "!cat logs/spark-Alexandre-org.apache.spark.deploy.master.Master-1-Alexandres-MBP.lan.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# note spark://Alexandres-MBP.lan:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.worker.Worker running as process 36853.  Stop it first.\r\n"
     ]
    }
   ],
   "source": [
    "! ./sbin/start-slave.sh spark://Alexandres-MBP.lan:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Alexandre/workstation/spark-2.0.0-bin-hadoop2.7\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bin/spark-submit:\n",
    "basically does this...\n",
    "exec \"${SPARK_HOME}\"/bin/spark-class org.apache.spark.deploy.SparkSubmit \"$@\"\n",
    "\n",
    "org.apache.spark.deploy.SparkSubmit:\n",
    "https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala\n",
    "\n",
    "https://spark.apache.org/docs/1.5.2/api/python/_modules/pyspark/streaming/kafka.html#KafkaUtils\n",
    "\n",
    "trying this instead\n",
    "$./bin/spark-submit examples/src/main/python/streaming/kafka_wordcount.py localhost:2181 test\n",
    "\n",
    "add-ons\n",
    "http://search.maven.org/#search%7Cga%7C1%7Cspark-streaming-kafka-0-8-assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/Alexandre/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/Alexandre/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/Alexandre/Workstation/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "org.apache.spark#spark-streaming-kafka-0-8_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.0 in central\n",
      "\tfound org.apache.kafka#kafka_2.11;0.8.2.1 in central\n",
      "\tfound org.scala-lang.modules#scala-xml_2.11;1.0.2 in central\n",
      "\tfound com.yammer.metrics#metrics-core;2.2.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.16 in central\n",
      "\tfound org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 in central\n",
      "\tfound com.101tec#zkclient;0.3 in central\n",
      "\tfound log4j#log4j;1.2.17 in central\n",
      "\tfound org.apache.kafka#kafka-clients;0.8.2.1 in central\n",
      "\tfound net.jpountz.lz4#lz4;1.3.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.2.4 in central\n",
      "\tfound org.apache.spark#spark-tags_2.11;2.0.0 in central\n",
      "\tfound org.scalatest#scalatest_2.11;2.2.6 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      ":: resolution report :: resolve 1197ms :: artifacts dl 36ms\n",
      "\t:: modules in use:\n",
      "\tcom.101tec#zkclient;0.3 from central in [default]\n",
      "\tcom.yammer.metrics#metrics-core;2.2.0 from central in [default]\n",
      "\tlog4j#log4j;1.2.17 from central in [default]\n",
      "\tnet.jpountz.lz4#lz4;1.3.0 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;0.8.2.1 from central in [default]\n",
      "\torg.apache.kafka#kafka_2.11;0.8.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-8_2.11;2.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-tags_2.11;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.scala-lang.modules#scala-parser-combinators_2.11;1.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-xml_2.11;1.0.2 from central in [default]\n",
      "\torg.scalatest#scalatest_2.11;2.2.6 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.16 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.2.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   0   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/47ms)\n",
      "jupyter: '/Users/Alexandre/Workstation/spark-2.0.0-bin-hadoop2.7/examples/src/main/python/streaming/direct_kafka_wordcount.py' is not a Jupyter command\n"
     ]
    }
   ],
   "source": [
    "# this works\n",
    "!./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.0 examples/src/main/python/streaming/direct_kafka_wordcount.py localhost:9092 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"PythonStreamingDirectKafkaWordCount\")\n",
    "ssc = StreamingContext(sc, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n",
      "\n",
      "  1. Include the Kafka library and its dependencies with in the\n",
      "     spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8:2.0.0 ...\n",
      "\n",
      "  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n",
      "     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-8-assembly, Version = 2.0.0.\n",
      "     Then, include the jar in the spark-submit command as\n",
      "\n",
      "     $ bin/spark-submit --jars <spark-streaming-kafka-0-8-assembly.jar> ...\n",
      "\n",
      "________________________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-c9ad680896d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDirectStream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"metadata.broker.list\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"localhost:9092\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Alexandre/workstation/spark-2.0.0-bin-hadoop2.7/python/pyspark/streaming/kafka.pyc\u001b[0m in \u001b[0;36mcreateDirectStream\u001b[0;34m(ssc, topics, kafkaParams, fromOffsets, keyDecoder, valueDecoder, messageHandler)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmessageHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mhelper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         jfromOffsets = dict([(k._jTopicAndPartition(helper),\n",
      "\u001b[0;32m/Users/Alexandre/workstation/spark-2.0.0-bin-hadoop2.7/python/pyspark/streaming/kafka.pyc\u001b[0m in \u001b[0;36m_get_helper\u001b[0;34m(sc)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkafka\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKafkaUtilsPythonHelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"'JavaPackage' object is not callable\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "source": [
    "kvs = KafkaUtils.createDirectStream(ssc, ['test'], {\"metadata.broker.list\": \"localhost:9092\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
